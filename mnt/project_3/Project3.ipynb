{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27d1b87f-dede-404a-944f-5ed2a6237b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import (\n",
    "    lead, lag, unix_timestamp, \n",
    "    when, col, sum, udf, avg\n",
    ")\n",
    "from pyspark.sql.types import StringType\n",
    "import pyspark\n",
    "\n",
    "from delta import *\n",
    "from delta.tables import *\n",
    "\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, Word2Vec\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.sql.functions import col, lower, regexp_replace\n",
    "import re\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import lit\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "builder = (\n",
    "    SparkSession.builder.appName(\"Refr\")\n",
    "    .config(\"spark.sql.extensions\", \n",
    "            \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \n",
    "            \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    ")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a810950-ea68-4329-ad37-e0de7f7e68ab",
   "metadata": {},
   "source": [
    "Data discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfaf44a8-d048-44f2-9e25-c097c42a6bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- abstract: string (nullable = true)\n",
      " |-- authors: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- n_citation: long (nullable = true)\n",
      " |-- references: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- venue: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "|            abstract|             authors|                  id|n_citation|          references|               title|               venue|year|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "|The purpose of th...|[Makoto Satoh, Ry...|00127ee2-cb05-48c...|         0|[51c7e02e-f5ed-43...|Preliminary Desig...|international con...|2013|\n",
      "|This paper descri...|[Gareth Beale, Gr...|001c58d3-26ad-46b...|        50|[10482dd3-4642-41...|A methodology for...|visual analytics ...|2011|\n",
      "|This article appl...|[Altaf Hossain, F...|001c8744-73c4-4b0...|        50|[2d84c0f2-e656-4c...|Comparison of GAR...|pattern recogniti...|2009|\n",
      "|                NULL|[Jea-Bum Park, By...|00338203-9eb3-40c...|         0|[8c78e4b0-632b-42...|Development of Re...|                    |2011|\n",
      "|                NULL|[Giovanna Guerrin...|0040b022-1472-4f7...|         2|                NULL|Reasonig about Se...|                    |1998|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+--------------------+--------------------+------------------+-------------------------------------+--------------------+------------------+\n",
      "|summary|            abstract|                  id|        n_citation|                                title|               venue|              year|\n",
      "+-------+--------------------+--------------------+------------------+-------------------------------------+--------------------+------------------+\n",
      "|  count|             2548532|             3079007|           3079007|                              3079007|             3079007|           3079007|\n",
      "|   mean|                NULL|                NULL|35.220902713114974|                                 NULL|                NULL|2007.7665994263734|\n",
      "| stddev|                NULL|                NULL|157.70065110545156|                                 NULL|                NULL|  7.81653849862412|\n",
      "|    min|\u000e\u000f\u0010\u0010\u0011 \u0012\u0013\u0010\u0014 \u000e\u000f\u0010\u0011 \u0012...|000000b8-7f59-49a...|                 0|                 ! and ? ‚Äì Storage...|                    |              1936|\n",
      "|    max|ùëòÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ-Anonymo...|ffffe1e6-981e-4cf...|             73362|ÔßùÁî®ÈõôË™ûÂ≠∏Ë°ìÂêçË©ûÂ∫´ÊäΩÂèñ‰∏≠Ëã±Â≠óË©û‰∫íË≠Ø...|worst-case execut...|              2018|\n",
      "+-------+--------------------+--------------------+------------------+-------------------------------------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Schema\n",
    "data = spark.read.format(\"json\").option(\"inferSchema\",\"true\").load(\"dblp-ref-0.json\")\n",
    "schema = data.schema\n",
    "\n",
    "for i in range(1, 4):\n",
    "    file_name = f\"dblp-ref-{i}.json\"\n",
    "    df = spark.read.json(file_name, schema=schema)\n",
    "    data = data.union(df)\n",
    "\n",
    "data.printSchema()\n",
    "data.show(5)\n",
    "data.describe().show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f88c16b-a5db-4dfd-808b-4098096c241e",
   "metadata": {},
   "source": [
    "Remove non-English titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27ee5d62-1195-41fb-8717-9a248e82854a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import BooleanType\n",
    "\n",
    "data = data.na.drop(subset=[\"title\"])\n",
    "\n",
    "# Ascii\n",
    "def is_ascii(s):\n",
    "    return all(ord(c) < 128 for c in s)\n",
    "\n",
    "# UDF\n",
    "is_ascii_udf = udf(is_ascii, BooleanType())\n",
    "\n",
    "\n",
    "# Filter\n",
    "df_eng = data.filter(is_ascii_udf(data['title']))\n",
    "\n",
    "#df_eng.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bf0518-4a3c-4b51-a7fe-7dc01e5f973f",
   "metadata": {},
   "source": [
    "Remove stop words and lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03903bfa-606c-4857-a6d8-f9d6e87dfa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "\n",
    "# Lower\n",
    "df_eng = df_eng.withColumn(\"title\", lower(col(\"title\")))\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"title\", outputCol=\"words\")\n",
    "df_eng = tokenizer.transform(df_eng)\n",
    "\n",
    "# Word list\n",
    "custom_stop_words = ['doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure','rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', 'al.', 'Elsevier', 'PMC', 'CZI', 'www']\n",
    "\n",
    "# Stop words\n",
    "stop_words = StopWordsRemover().getStopWords()\n",
    "\n",
    "stop_words = stop_words + custom_stop_words\n",
    "\n",
    "# StopWordRemover\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\", stopWords=stop_words)\n",
    "\n",
    "df_eng = remover.transform(df_eng)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a065ed2b-1c05-4dad-a3c1-e2e446e154c0",
   "metadata": {},
   "source": [
    "Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41212811-ca1d-4d79-8683-74e2c6b81d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.sql.functions import lower, col\n",
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "# toString\n",
    "df_eng = df_eng.withColumn(\"filtered\", concat_ws(\" \", col(\"filtered\")))\n",
    "\n",
    "# Regex\n",
    "pattern = \"[!()-\\[\\]{};:'\\\"\\,<>./?@#$%^&*_~]\"\n",
    "\n",
    "# Regex transform\n",
    "tokenizer = RegexTokenizer(inputCol=\"filtered\", outputCol=\"low_filtered\", pattern=pattern, gaps=True)\n",
    "df_eng = tokenizer.transform(df_eng)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b8e80a0-708b-40ee-9f6d-48d431b74b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_eng.select(\"low_filtered\").show(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb10bd9b-6160-4033-80da-937d7168b29d",
   "metadata": {},
   "source": [
    "Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1355b65f-911f-4a7b-bb87-575a382dbda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "df_eng = df_eng.repartition(10)\n",
    "\n",
    "word2Vec = Word2Vec(vectorSize=2, minCount=5, inputCol=\"low_filtered\", outputCol=\"result\")\n",
    "\n",
    "# Fit the model\n",
    "model = word2Vec.fit(df_eng)\n",
    "\n",
    "# Transform the DataFrame\n",
    "result = model.transform(df_eng)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4866a07b-0cb7-4503-933e-20701dadbec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "johnson: 1.0\n",
      "s f: 0.9999999403953552\n",
      "cp : 0.9999998807907104\n",
      "spur: 0.9999998807907104\n",
      "singly: 0.9999998807907104\n"
     ]
    }
   ],
   "source": [
    "# Find synonyms\n",
    "synonyms = model.findSynonyms('psychology', 5)\n",
    "for word, cosine_distance in synonyms.collect():\n",
    "    print(\"{}: {}\".format(word, cosine_distance))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50448de-fe95-4667-a814-08135c1a2c40",
   "metadata": {},
   "source": [
    "Elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e30622f7-e97f-4c28-b1e9-f632a99f8198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(k=2, inputCol=\"result\", outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(result)\n",
    "result = model.transform(result)\n",
    "\n",
    "# Apply KMeans\n",
    "kmeans = KMeans(k=3, seed=1, featuresCol=\"pcaFeatures\")  \n",
    "model = kmeans.fit(result.select('pcaFeatures'))\n",
    "predictions = model.transform(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb19a343-1b08-44b9-a7d7-0b8a49d366de",
   "metadata": {},
   "source": [
    "Search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99d802ec-1318-4374-be45-541d9d0f3378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a UDF to compute cosine similarity\n",
    "def cosine_similarity(v1, v2):\n",
    "    return float(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))\n",
    "\n",
    "cosine_similarity_udf = udf(cosine_similarity, FloatType())\n",
    "\n",
    "# Get the result of the filter operation\n",
    "filtered_result = result.filter(result.title == 'crowdsourcing as lego: unpacking the building blocks of crowdsourcing collaboration processes').select('result').first()\n",
    "\n",
    "# Check if filtered_result is not None before accessing its elements\n",
    "if filtered_result is not None:\n",
    "    input_vector = filtered_result[0].toArray().tolist()  # Convert DenseVector to list\n",
    "else:\n",
    "    print(\"No rows found\")\n",
    "\n",
    "\n",
    "# Calculate cosine similarity with all other papers\n",
    "result = result.withColumn('similarity', cosine_similarity_udf('result', lit(input_vector)))\n",
    "\n",
    "# Recommend the top N papers\n",
    "recommendations = result.orderBy('similarity', ascending=False).limit(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59e5b43f-0495-4952-9466-9103cc539cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|            abstract|             authors|                  id|n_citation|          references|               title|               venue|year|               words|            filtered|        low_filtered|              result|         pcaFeatures|similarity|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|                NULL|[Helga Naessens, ...|139f0297-bc9c-46a...|        50|                NULL|generating hasse ...|                    |1999|[generating, hass...|generating hasse ...|[generating hasse...|[0.04697725549340...|[-0.0064916757243...|       NaN|\n",
      "|                NULL|[Lilia Efimova, J...|a43973b0-0206-4b5...|        12|                NULL|converging knowle...|Journal of Univer...|2003|[converging, know...|converging knowle...|[converging knowl...|[0.05435653030872...|[-0.0333978667910...|       NaN|\n",
      "|                NULL|[Jeang-Kuo Chen, ...|15368be8-179c-45b...|         0|                NULL|dynamic access of...|distributed multi...|2008|[dynamic, access,...|dynamic access xm...|[dynamic access x...|[0.15248693525791...|[-0.1787063197370...|       NaN|\n",
      "|In this paper we ...|[Zs√≤fia Ruttkay, ...|1c82eafe-4e79-4ee...|        10|[58b6f697-1db3-43...|reusable gestures...|intelligent virtu...|2003|[reusable, gestur...|reusable gestures...|[reusable gesture...|           [0.0,0.0]|           [0.0,0.0]|       NaN|\n",
      "|                NULL|[Philippe Collet,...|1ccf0476-e1fe-4a3...|        13|                NULL|efficient impleme...|Logiciel, Base De...|1999|[efficient, imple...|efficient impleme...|[efficient implem...|           [0.0,0.0]|           [0.0,0.0]|       NaN|\n",
      "|                NULL|[Shota Okujava, U...|8f3a1eab-fa42-41c...|        50|                NULL|wirtschaftlichkei...|Information Techn...|2006|[wirtschaftlichke...|wirtschaftlichkei...|[wirtschaftlichke...|           [0.0,0.0]|           [0.0,0.0]|       NaN|\n",
      "|HTTPS is the stan...|[Jason Gionta, Pe...|4d490205-d35c-4e4...|        50|[2893cd6a-84e6-48...|ihttp: efficient ...|applied cryptogra...|2012|[ihttp:, efficien...|ihttp: efficient ...|[ihttp,  efficien...|           [0.0,0.0]|           [0.0,0.0]|       NaN|\n",
      "|Multicore hardwar...|[Thomas Karcher, ...|4ea73c9c-2cd6-436...|        50|[011273cc-312f-4b...|run-time automati...|european conferen...|2011|[run-time, automa...|run-time automati...|[run, time automa...|[0.12357930094003...|[-0.1383012334471...|       NaN|\n",
      "|Lane-border detec...|[Ali Al-Sarraf, B...|3f4f495c-828c-4db...|        50|[5b37a4ce-df21-43...|ground truth and ...|international con...|2014|[ground, truth, a...|ground truth perf...|[ground truth per...|           [0.0,0.0]|           [0.0,0.0]|       NaN|\n",
      "|                NULL|[Il-Kwon Jeong, I...|a28e25db-19bc-483...|         0|                NULL|a new spring mode...|                    |2003|[a, new, spring, ...|new spring model ...|[new spring model...|           [0.0,0.0]|           [0.0,0.0]|       NaN|\n",
      "|                NULL|[Zhen Ye, Cheng-C...|b47f3ea7-dfd7-4ae...|        15|[452ea6a8-3fa2-4b...|unsupervised mult...|                    |2002|[unsupervised, mu...|unsupervised mult...|[unsupervised mul...|           [0.0,0.0]|           [0.0,0.0]|       NaN|\n",
      "|                NULL|[Jivika Govil, Ji...|fa7787d3-e107-422...|         0|                NULL|management inform...|                    |2008|[management, info...|management inform...|[management infor...|           [0.0,0.0]|           [0.0,0.0]|       NaN|\n",
      "|                NULL|[Maxim Schnjakin,...|b9023971-e3f1-4d6...|        10|[1e19e8da-c281-49...|implementation of...|grid and pervasiv...|2013|[implementation, ...|implementation cl...|[implementation c...|[0.03101795911788...|[0.01567458577770...|       NaN|\n",
      "|                NULL|[Mercedes R. Fern...|3795fab1-a234-4ab...|         0|                NULL|platforms used by...|                    |2008|[platforms, used,...|platforms mexican...|[platforms mexica...|[0.10871306061744...|[-0.0667957335821...|       NaN|\n",
      "|                NULL|[Chengyuan Ma, Yu...|17e8f082-5d44-4d8...|        22|                NULL|a study on detect...|conference of the...|2006|[a, study, on, de...|study detection b...|[study detection ...|           [0.0,0.0]|           [0.0,0.0]|       NaN|\n",
      "|                NULL|[Luigi V. Mancini...|c364bac4-e2b3-451...|        50|[136c4780-2f25-40...|towards a theory ...|                    |1988|[towards, a, theo...|towards theory re...|[towards theory r...|           [0.0,0.0]|           [0.0,0.0]|       NaN|\n",
      "|                NULL|     [Atsushi Ohori]|bdc00a49-581c-409...|         0|                NULL|logical analysis ...|asian symposium o...|2000|[logical, analysi...|logical analysis ...|[logical analysis...|           [0.0,0.0]|           [0.0,0.0]|       NaN|\n",
      "|                NULL|[Rita Suzana Pita...|41f925ec-11ce-4b4...|        50|                NULL|an mda-edoc based...|international con...|2005|[an, mda-edoc, ba...|mda-edoc based de...|[mda, edoc based ...|[-0.0139422202482...|[0.00466230672827...|       NaN|\n",
      "|                NULL|[Omar Walid Llore...|00837359-8230-41c...|         0|                  []|ipv6 deployment, ...|                    |2006|[ipv6, deployment...|ipv6 deployment, ...|[ipv,  deployment...|[0.18093882997830...|[-0.1316011689224...|       NaN|\n",
      "|Using secondary d...|[Shirish C. Sriva...|bfe31e99-6d4d-4f0...|        52|[05a6b91f-9d0e-48...|the relationship ...|Communications of...|2008|[the, relationshi...|relationship e-go...|[relationship e, ...|[-0.0309975693623...|[0.03963550299372...|       NaN|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recommendations.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a72965f-e163-45c8-8a30-fd1940434e35",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced35f04-803c-4050-84ed-e42a7534c597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Define a UDF to compute cosine similarity\n",
    "def cosine_similarity(v1, v2):\n",
    "    return float(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))\n",
    "\n",
    "cosine_similarity_udf = udf(cosine_similarity, FloatType())\n",
    "\n",
    "# Get the unique titles in the DataFrame\n",
    "unique_titles = result.select('title').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Iterate over the unique titles\n",
    "for title in unique_titles:\n",
    "    # Get the result of the filter operation\n",
    "    filtered_result = result.filter(result.title == title).select('result').first()\n",
    "\n",
    "    # Check if filtered_result is not None before accessing its elements\n",
    "    if filtered_result is not None:\n",
    "        input_vector = Vectors.dense(filtered_result[0])  # Convert numpy array to PySpark Vector\n",
    "        print(f\"Found a non-empty result for title: {title}\")\n",
    "        break\n",
    "    else:\n",
    "        print(f\"No rows found for title: {title}\")\n",
    "\n",
    "# Check the dimensionality of the vectors\n",
    "vector_length = len(result.select('result').first()[0])\n",
    "input_vector_length = len(input_vector)\n",
    "\n",
    "if vector_length != input_vector_length:\n",
    "    print(f\"Vector dimensions do not match: input_vector has {input_vector_length} dimensions, but vectors in 'result' column have {vector_length} dimensions.\")\n",
    "else:\n",
    "    # Calculate cosine similarity with all other papers\n",
    "    result = result.withColumn('similarity', cosine_similarity_udf('result', lit(input_vector.tolist())))  # Convert DenseVector to list\n",
    "\n",
    "    # Recommend the top N papers\n",
    "    recommendations = result.orderBy('similarity', ascending=False).limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1ccaf9-c086-4348-a1c7-b1de7a533f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_titles = result.select('id').distinct()\n",
    "unique_titles.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ba5196-99da-42b2-9029-6c5c1a97c138",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
